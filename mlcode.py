# -*- coding: utf-8 -*-
"""mlcode.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Gg1ZFKzLOFdXqDwU93kyvahQI24MsPJf

# Data cleaning and merging

### Dependencies
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline
import warnings
import missingno as msno
from functools import reduce
warnings.filterwarnings('ignore')

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.metrics import f1_score
from sklearn.model_selection import KFold
from sklearn.metrics import roc_auc_score
from sklearn import metrics
import lightgbm as lgb
from lightgbm import LGBMClassifier

seed = 42

# This path will be active after the launch of the hackathon
teamname = 'trachack-project-groups-7-unavarra'
data_folder='s3://tf-trachack-data/212/'
# change root_folder to your team's root folder
# s3://tf-trachack-notebooks/<this should be replaced by team name as provided in EMAIL>/jupyter/jovyan/
root_folder='s3://tf-trachack-notebooks/'+teamname+'/jupyter/jovyan/'

def get_unique_values(df):
    var_list = df.columns.to_list()
    for var in var_list:
        print(var, df[var].unique())
        
def get_catplot(df):
    var_list = df_phone_info.columns.to_list()
    for var in var_list:
        g = sns.catplot(x = str(var), kind="count", data = df)
        g.set_xticklabels(rotation=90)
        g.fig.suptitle(str(var) +'_categories', fontsize=16)
        
def make_binary(df, var):
    if len(list(df[var].unique()))==2:
        var_dict =  {list(df[var].unique())[0]: 0, list(df[var].unique())[1]: 1}
        df[var] = df[var].map(var_dict)
    else:
        var_dict =  {list(df[var].unique())[0]: 0}
        df[var] = df[var].map(var_dict)
        
def make_dummies(df):
    dummed_features = []
    for col_name in df.columns:
        if(df[col_name].dtype == 'object'):
            j = pd.get_dummies(df[col_name], prefix = str(col_name), drop_first = False)
            df = pd.concat([df, j], axis=1)
            dummed_features.append(col_name)
    df = df[df.columns.difference(dummed_features)]
    return df

def turn_date_to_number(df, date_var):
    df[date_var] = pd.to_datetime(df[date_var])
    df[date_var+'_standard'] = pd.to_datetime(df[date_var])
    df['year'] = df[date_var+'_standard'].apply(lambda row: row.year)
    df['month'] = df[date_var+'_standard'].apply(lambda row: row.month)
    df['day'] = df[date_var+'_standard'].apply(lambda row: row.day)
    df['year'] = df['year']*10000
    df['month'] = df['month']*100
    df[date_var+'_continuous'] = df['year'] + df['month'] + df['day']
    var_to_drop = date_var+'_standard'
    df.drop(var_to_drop, axis= 1, inplace = True)
    df.drop('year', axis= 1, inplace = True)
    df.drop('month', axis= 1, inplace = True)
    df.drop('day', axis= 1, inplace = True)
    
def drop_dates(df):
    for col_name in df.columns:
        if(df[col_name].dtype == 'datetime64[ns]'):
            df.drop(col_name, axis = 1, inplace = True)

def mismatch_df(df1, df2, ids): #tienen que tener igual columnas
    keys = [ids]
    col_list = [col for col in df1.columns if col not in keys]
    sel_cols = keys.copy()
    sel_cols.extend(col_list)
    df1_index = df1[sel_cols].set_index(keys)
    df2_index = df2[sel_cols].set_index(keys)
    # make an equivalency boolean mask
    df1_index.update(df2_index)
    mask = np.equal(df1[col_list].values, df1_index.values).all(axis=1)
    # slice df1 with mask
    Mismatch = df1[~mask]
    return Mismatch    

#funcion devuelve ids entre dos dataframes
def match_ids1(df1, df2):
    set1 = set(df1.line_id.unique().tolist())
    set2 = set(df2.line_id.unique().tolist())
    matched_ids = set1.intersection(set2)
    print(f"num of ids df_1: {len(set1)} , num of ids df_2: {len(set2)}, num of matched ids: {len(matched_ids)}")
    return matched_ids

#funcion devuelve ids entre dataframe y una lista de ids
def match_ids2(df1, list_ids):
    set1 = set(df1.line_id.unique().tolist())
    matched_ids = set1.intersection(set(list_ids))
    print(f"num of ids df_1: {len(set1)} , num of ids in list: {len(list_ids)}, num of matched ids: {len(matched_ids)}")
    return matched_ids

def information (df):
    print('Train Dataset Infomarion')
    print ("Rows     : " ,df.shape[0])
    print ("Columns  : " ,df.shape[1])
    print ("\nFeatures : \n" ,df.columns.tolist())
    print ("\nMissing values :  ", df.isnull().sum().values.sum())
    print ("\nUnique values :  \n", df.nunique())
    
def duplicates (df):
    df.drop_duplicates(keep='first',inplace=True)
    

def index(df):
    df.set_index('line_id', inplace = True)
    
def boolean (df, var_bool):
    d = {'no': 0, 'yes': 1}
    df[var_bool] = df[var_bool].map(d)
    return df.head()

def dummies (df):
    dummed_features = []
    for col_name in df.columns:
        if(df[col_name].dtype == 'object'):
            j = pd.get_dummies(df[col_name], prefix = str(col_name), drop_first = False)
            df = pd.concat([df, j], axis=1)
            dummed_features.append(col_name)
    df = df[df.columns.difference(dummed_features)]
    return df

def prop_values (df):
    for col_name in df.columns:
        if df[col_name].dtype.name == 'object':
            print(df[col_name].name.upper(), ':')
            print('NA:', round(1-df[col_name].count()/len(df), 4))
            print(round((df[col_name].count()/len(df))*(df[col_name].value_counts(normalize=True)),4))
            print("\n\n")
    return df
            
def dates_range (df):
    for col_name in df.columns:
        if df[col_name].dtype.name == 'datetime64[ns]':
            print(df[col_name].name.upper(), ':')
            print('Min:', df[col_name].min())
            print('Max:', df[col_name].max())
            print("\n\n")
        elif df[col_name].dtype.name == 'timedelta64[ns]':
            print(df[col_name].name.upper(), ':')
            print('Min:',df[col_name].min())
            print('Max:',df[col_name].max())
    return df

def preprocessing_customer_info(df):
    df['min_activation_date'] = '2012-02-28'
    df['min_activation_date'] = pd.to_datetime(df['min_activation_date'])
    df['first_activation_date'] = pd.to_datetime(df['first_activation_date'])
    df['days_since_first_activation'] = (df['first_activation_date'] - df['min_activation_date']).dt.days
    duplicates(df)
    turn_date_to_number(df, 'first_activation_date')
    turn_date_to_number(df, 'redemption_date')
    drop_dates(df)
    index(df)
    plan_list = ['plan 2', 'plan 3', 'plan 4']
    df.loc[df['plan_name'].isin(plan_list), 'plan_name'] = 'Other'
    plan_subtype_list = ['ADD ON']
    df.loc[df['plan_subtype'].isin(plan_subtype_list), 'plan_subtype'] = np.nan
    carrier_list = ['carrier 2', 'carrier 3']
    df.loc[df['carrier'].isin(carrier_list), 'carrier'] = 'Other'
    df = dummies(df)
    df.drop('redemption_date_continuous', axis = 1, inplace = True)
    df.reset_index(inplace = True)
    return df

def preprocessing_phone_info(df):
    #Let's remove the rows that contain all missing values for all variables
    keep_cols = df.columns.to_list()
    keep_cols = keep_cols[1:]
    df['all_NAs'] = df[keep_cols].isnull().apply(lambda x: all(x), axis = 1)
    df_all_nas = df[df['all_NAs'] == True]
    df_all_nas.set_index('line_id', inplace = True)
    df_all_nas = df_all_nas.add_suffix('_to_delete')
    df_no_all_nas = df[df['all_NAs'] == False]

    #cpu_cores
    df_no_all_nas['cpu_cores'].fillna('Other', inplace = True)
    cpu_cores_dict = df_no_all_nas.cpu_cores.value_counts().to_dict()
    list_cpu_cores = [k for k,v in cpu_cores_dict.items() if v <= 0.05*len(df_no_all_nas)]
    for model in list_cpu_cores:
        df_no_all_nas.loc[df_no_all_nas['cpu_cores'] == model, 'cpu_cores'] = 'Other'

    #internal storage capacity
    df_no_all_nas['internal_storage_capacity'].fillna('Other', inplace = True)
    df_no_all_nas.loc[df_no_all_nas['internal_storage_capacity'].str.endswith('16'), 'internal_storage_capacity'] = 'up_to_64gb'
    df_no_all_nas.loc[df_no_all_nas['internal_storage_capacity'].str.endswith('32'), 'internal_storage_capacity'] = 'up_to_64gb'
    df_no_all_nas.loc[df_no_all_nas['internal_storage_capacity'].str.endswith('64'), 'internal_storage_capacity'] = 'up_to_64gb'
    df_no_all_nas.loc[df_no_all_nas['internal_storage_capacity'].str.endswith('128'), 'internal_storage_capacity'] = 'up_to_128gb'
    df_no_all_nas.loc[df_no_all_nas['internal_storage_capacity'].str.endswith('256'), 'internal_storage_capacity'] = 'up_to_256gb'
    df_no_all_nas.loc[df_no_all_nas['internal_storage_capacity'].str.endswith('512'), 'internal_storage_capacity'] = 'up_to_512gb'

    internal_storage_capacity_dict = df_no_all_nas.internal_storage_capacity.value_counts().to_dict()
    list_internal_storage_capacity = [k for k,v in internal_storage_capacity_dict.items() if v <= 0.05*len(df_no_all_nas)]

    for model in list_internal_storage_capacity:
        df_no_all_nas.loc[df_no_all_nas['internal_storage_capacity'] == model, 'internal_storage_capacity'] = 'Other'

    #total_ram
    df_no_all_nas['total_ram'].fillna('Other', inplace = True)
    df_no_all_nas.loc[df_no_all_nas['total_ram'].str.endswith('1024'), 'total_ram'] = 'up_to_2gb'
    df_no_all_nas.loc[df_no_all_nas['total_ram'].str.endswith('2048'), 'total_ram'] = 'up_to_2gb'
    df_no_all_nas.loc[df_no_all_nas['total_ram'].str.endswith('3072'), 'total_ram'] = 'up_to_4gb'
    df_no_all_nas.loc[df_no_all_nas['total_ram'].str.endswith('4096'), 'total_ram'] = 'up_to_4gb'
    df_no_all_nas.loc[df_no_all_nas['total_ram'].str.endswith('6144'), 'total_ram'] = '6gb_to_12gb'
    df_no_all_nas.loc[df_no_all_nas['total_ram'].str.endswith('8192'), 'total_ram'] = '6gb_to_12gb'
    df_no_all_nas.loc[df_no_all_nas['total_ram'].str.endswith('12288'), 'total_ram'] = '6gb_to_12gb'

    total_ram_dict = df_no_all_nas.total_ram.value_counts().to_dict()
    list_ram = [k for k,v in total_ram_dict.items() if v <= 0.05*len(df_no_all_nas)]

    for model in list_ram:
        df_no_all_nas.loc[df_no_all_nas['total_ram'] == model, 'total_ram'] = 'Other'

    #gsma_model_name
    df_no_all_nas.loc[df_no_all_nas['gsma_model_name'].str.startswith('SM-'), 'manufacturer'] = 'Samsung'
    df_no_all_nas.loc[df_no_all_nas['gsma_model_name'].str.startswith('LG'), 'manufacturer'] = 'LG'
    df_no_all_nas.loc[df_no_all_nas['gsma_model_name'].str.startswith('XT'), 'manufacturer'] = 'Motorola'

    manufacturer_dict = df_no_all_nas.manufacturer.value_counts().to_dict()
    list_manufacturer = [k for k,v in manufacturer_dict.items() if v <= 0.05*len(df_no_all_nas)]

    for model in list_manufacturer:
        df_no_all_nas.loc[df_no_all_nas['manufacturer'] == model, 'manufacturer'] = 'Other'
        df_no_all_nas['manufacturer'].fillna('Other', inplace = True)

    #os_name
    df_no_all_nas['os_name'].fillna('Other', inplace = True)
    os_name_dict = df_no_all_nas.os_name.value_counts().to_dict()
    list_os_name = [k for k,v in os_name_dict.items() if v <= 0.05*len(df_no_all_nas)]
    for model in list_os_name:
        df_no_all_nas.loc[df_no_all_nas['os_name'] == model, 'os_name'] = 'Other'

    #gsma_device_type
    df_no_all_nas['gsma_device_type'].fillna('Other', inplace = True)
    gsma_device_type_dict = df_no_all_nas.gsma_device_type.value_counts().to_dict()
    list_gsma_device_type = [k for k,v in gsma_device_type_dict.items() if v <= 0.05*len(df_no_all_nas)]
    for model in list_gsma_device_type:
        df_no_all_nas.loc[df_no_all_nas['gsma_device_type'] == model, 'gsma_device_type'] = 'Other'   

    #year_released
    df_no_all_nas['year_released'].fillna(df_no_all_nas['year_released'].median(), inplace=True)

    #keep only the relevant variables
    keep_vars_list = ['line_id', 'cpu_cores', 'gsma_device_type', 'os_name', 'internal_storage_capacity', 'total_ram', 'manufacturer', 'year_released']
    df_no_all_nas = df_no_all_nas[keep_vars_list]

    #set line_id as index
    df_no_all_nas.set_index('line_id', inplace = True)

    #make dummies
    df_no_all_nas = make_dummies(df_no_all_nas)

    #Concatenate the rest of line_ids that were deleted at first
    frames = [df_no_all_nas, df_all_nas]
    df_final = pd.concat(frames)
    
    columns_to_delete = list(df_all_nas.columns)

    #Get rid of the unnecessary columns and replace missing values
    df_final.drop(columns_to_delete, axis = 1, inplace = True)
    df_final.fillna(0, inplace = True)

    #Reset the index
    df_final.reset_index(inplace = True)
    
    return df_final    

def preprocessing_redemptions(df):
    #Drop unnecesary variables
    df.drop('redemption_type', axis = 1, inplace = True)
    df.drop('revenue_type', axis = 1, inplace = True)

    #Turn dates to numeric
    turn_date_to_number(df, 'redemption_date')

    #get rid of the original redemption date
    drop_dates(df)

    #Get the mode of the variables grouped by line_id. THIS WILL TAKE A WHILE TO RUN
    df = df.groupby(['line_id']).agg(lambda x:x.value_counts().index[0])

    #Get dummies
    channel_dict = df.channel.value_counts().to_dict()
    list_channel = [k for k,v in channel_dict.items() if v <= 0.05*len(df)]
    for model in list_channel:
        df.loc[df['channel'] == model, 'channel'] = 'Other' 
    df = make_dummies(df)
    
    return df

def preprocessing_reactivations(df):   

    #Turn date to continuous number
    turn_date_to_number(df, 'reactivation_date')
    drop_dates(df)
    #Get the mode or max of the variables grouped by line_id. THIS WILL TAKE A WHILE TO RUN
    df = df.groupby(['line_id']).agg(lambda x:x.value_counts().index[0])

    #Get dummies
    reactivation_channel_dict = df.reactivation_channel.value_counts().to_dict()
    list_reactivation_channel = [k for k,v in reactivation_channel_dict.items() if v <= 0.05*len(df)]
    for model in list_reactivation_channel:
        df.loc[df['reactivation_channel'] == model, 'reactivation_channel'] = 'Other' 
    df = make_dummies(df)

    #Reset index
    df.reset_index(inplace = True)
    
    return df

def preprocessing_lrp_points(df):

    #imputing the NAs
    total_quantity_mean=round(df["total_quantity"].median())
    quantity_mean=round(df["quantity"].median())
    df['quantity'].fillna(quantity_mean, inplace = True)
    df['total_quantity'].fillna(total_quantity_mean, inplace = True)

    #changing values types
    df['quantity'] = df['quantity'].astype(np.int64)
    df['total_quantity'] = df['total_quantity'].astype(np.int64)
    turn_date_to_number(df, 'update_date')
    drop_dates(df)
    make_binary(df, 'status')
    
    return df

def preprocessing_lrp_enrollment(df):    

    make_binary(df, 'lrp_enrolled')
    #duplicated 
    duplicate_ids_enroll = df[df['line_id'].duplicated()]
    #finding duplicated ids
    ids_enroll_dup = duplicate_ids_enroll.line_id.unique().tolist()
    df[df["line_id"].isin(ids_enroll_dup)].head(20)
    #Selecting recod with the last date
    df=df.groupby('line_id', as_index=False).max()
    turn_date_to_number(df, 'lrp_enrollment_date')
    drop_dates(df)
    
    return df 

def preprocessing_network_usage(df):

    turn_date_to_number(df, 'date')
    drop_dates(df)
    df.rename(columns = {'date_continuous': 'network_date_continuous'}, inplace=True)
    df['total_min_calls'] = df['voice_min_in'] + df['voice_min_out'] 
    df['total_mms'] = df['mms_in'] + df['mms_out'] 
    df = df.groupby('line_id', as_index=False).mean()
    #Let's get the max values of the variables for each user. THIS WILL TAKE A WHILE TO RUN
    network_subset = df.loc[:, df.columns != 'date']
    network_subset = network_subset.groupby(['line_id']).agg(lambda x: x.value_counts().index[0])
    network_subset = network_subset.add_prefix('max_')
    #Now let's add these new variables to the network usage df
    df = pd.merge(df, network_subset ,on='line_id',how='left')
    
    return df


def left_join(data1, data2, data3, data4, data5, data6, data7, data8):
    data_merge1 = pd.merge(data1,data2,on='line_id',how='left')
    data_merge2 = pd.merge(data_merge1,data3,on='line_id',how='left')
    data_merge3 = pd.merge(data_merge2,data4,on='line_id',how='left')
    data_merge4 = pd.merge(data_merge3,data5,on='line_id',how='left')
    data_merge5 = pd.merge(data_merge4,data6,on='line_id',how='left')
    data_merge6 = pd.merge(data_merge5,data7,on='line_id',how='left')
    #data_merge7 = pd.merge(data_merge6,data8,on='line_id',how='left')
    #data_merge8 = pd.merge(data_merge7,data9,on='line_id',how='left')
    df_final = pd.merge(data_merge6,data8,on='line_id',how='left')
    
    return df_final

def imputing_missing_values(df):
    
    df.quantity.fillna(0, inplace = True)
    df.status.fillna(1, inplace = True)
    df.total_quantity.fillna(0, inplace = True)
    df.lrp_enrolled.fillna(1, inplace = True)

    for col_name in df.columns:
        if(df[col_name].dtype == 'datetime64[ns]'):
            df.drop(col_name, axis = 1, inplace = True)
    df.set_index('line_id', inplace = True)

    #For now, let's remove the dates which have too many missin values
    df.drop('update_date_continuous', axis = 1, inplace = True)
    df.drop('lrp_enrollment_date_continuous', axis = 1, inplace = True)
    
    return df

def load_merge_data(path):
    
    if path == 'eval':
        print(f"Loading Test data...")
    elif path == 'dev':
        print(f"Loading Training data...")
        
    # Load datasets
    df_upgrades=pd.read_csv(data_folder+"data/"+path+"/upgrades.csv")
    df_phone_info=pd.read_csv(data_folder+"data/"+path+"/phone_info.csv")
    df_redemptions=pd.read_csv(data_folder+"data/"+path+"/redemptions.csv")
    df_reactivations=pd.read_csv(data_folder+"data/"+path+"/reactivations.csv")
    df_points=pd.read_csv(data_folder+"data/"+path+"/lrp_points.csv")
    df_enrollment=pd.read_csv(data_folder+"data/"+path+"/lrp_enrollment.csv")
    df_network_usage = pd.read_csv(data_folder+"data/"+path+"/network_usage_domestic.csv")
    df_customer_info=pd.read_csv(data_folder+"data/"+path+"/customer_info.csv")
    print("Data Loaded")
    print("WARNING: it can take 25 min \n")
    

    ### Upgrades
    #information(df_upgrades)
    
    duplicates(df_upgrades)
    turn_date_to_number(df_upgrades, 'date_observed')
    drop_dates(df_upgrades)
    if path == "dev":
        index(df_upgrades)
        boolean(df_upgrades, 'upgrade')
        df_upgrades.reset_index(inplace = True)
    
    print("Processing Customer Info Data ...")
    df_customer_info = preprocessing_customer_info(df_customer_info)
    print("Processing Phone Info Data ...")
    df_phone_info = preprocessing_phone_info(df_phone_info)
    print("Processing Redemptions Data ...")
    df_redemptions = preprocessing_redemptions(df_redemptions)
    print("Processing Reactivations Data ...")
    df_reactivations = preprocessing_reactivations(df_reactivations)
    print("Processing Points Data ...")
    df_points = preprocessing_lrp_points(df_points)
    print("Processing Enrollment Data ...")
    df_enrollment = preprocessing_lrp_enrollment(df_enrollment)
    print("Processing Network Usage Data ...")
    df_network_usage = preprocessing_network_usage(df_network_usage)
    
    print("Merging All Data ...")
    df_merged = left_join(df_upgrades,df_network_usage, df_customer_info, df_redemptions, df_reactivations , df_phone_info, 
              df_enrollment, df_points)
    print("--- Merge Done --- \n")
    
    df_imputed = imputing_missing_values(df_merged)
    df_final = df_imputed.fillna(df_imputed.median())
    
    return df_final

"""# Model Validation"""

df_test = load_merge_data('eval')
df_train = load_merge_data('dev')

data_test = df_test
data_train = df_train

print(data_test.shape)
print(data_train.shape)

"""# Model Validation"""

data_train = data_train.rename(columns={'plan_subtype_[NULL]': 'plan_subtype_NULL'})
data_test = data_test.rename(columns={'plan_subtype_[NULL]': 'plan_subtype_NULL'})

target = 'upgrade'
features = [c for c in data_train.columns if c not in ['upgrade']]
y = data_train['upgrade'].values
X = data_train.loc[:, data_train.columns !='upgrade']
print(np.shape(X), np.shape(y))

X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.3,
                                                                    random_state=seed, shuffle= True)
print(len(X_train), len(y_train) ,'\n' ,len(X_valid), len(y_valid))

kf = KFold(n_splits = 5, random_state = seed, shuffle = True)

oof_preds = np.zeros(data_train.shape[0])
test_preds_prob = np.zeros(data_test.shape[0])
predic_cross  = np.zeros(len(data_test))
#feature_importance_df = pd.DataFrame()
n_fold = 0

for train_idx, valid_idx in kf.split(X_train, y_train):
    train_x, train_y = X_train.iloc[train_idx],y_train[train_idx]

    model =  LGBMClassifier(
                 seed = seed,
                 subsample = 0.4775,
                 n_estimators = 754,
                 feature_fraction = 0.76,
                 max_depth = 11,
                 learning_rate = 0.141,
                 reg_lambda = 0.05525,
                 reg_alpha = 0.045,
                 num_leaves = 66,
                 min_child_weight = 22,
                 objective='binary',
                 metric='auc',
                 feval=None) 
                    

    model.fit(train_x, train_y, eval_set=[(train_x, train_y), (X_valid, y_valid)], 
                    eval_metric= 'auc', verbose= 50, early_stopping_rounds= 50)

    predic_cross = model.predict(X_valid, num_iteration=model.best_iteration_)

    n_fold = n_fold + 1

f1 = f1_score(y_valid, predic_cross, average='micro') 
print(f"-------------------------------------------------------\n")
print(f"Scores F1:{f1}" )
print(f"-------------------------------------------------------")

"""# Model Prediction"""

model =  LGBMClassifier(
                 seed = seed,
                 subsample = 0.4775,
                 n_estimators = 754,
                 feature_fraction = 0.76,
                 max_depth = 11,
                 learning_rate = 0.141,
                 reg_lambda = 0.05525,
                 reg_alpha = 0.045,
                 num_leaves = 66,
                 min_child_weight = 22,
                 objective='binary',
                 metric='auc',
                 feval=None)                                

    
model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_valid, y_valid)], 
                eval_metric= 'auc', verbose= 50, early_stopping_rounds= 50)

predictions = model.predict(data_test, num_iteration=model.best_iteration_)

final_predicitions = pd.DataFrame(predictions, columns = ['upgrade_predictions'], index = data_test.index)
final_predicitions.head()

final_predicitions['upgrade_predictions'].value_counts()

submission_path=root_folder+"submission/2021-04-25.csv"
final_predicitions.to_csv(submission_path,header=True,index=False)
print(f"submission saved to {submission_path}")